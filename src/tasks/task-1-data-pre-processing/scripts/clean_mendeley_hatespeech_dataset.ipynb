{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0a9f3ce",
   "metadata": {},
   "source": [
    " # <span style=\"font-family: TrebucImport all required moduleshet MS; font-weight:bold;font-size:1.5em;color:#00b3e5;\">  Preprocessing script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6547ca7",
   "metadata": {},
   "source": [
    "## <span style=\"font-family: Trebuchet MS; font-weight:bold;font-size:0.7em;color:darkorange;\"> The raw data is available in the link https://data.mendeley.com/datasets/9sxpkmm8xn. Since it is more than 2GB data, no matter how you search it will not be available in this git repo. So before running this sript - download the zip, extract and have data_huang_devansh.csv available with you. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0304b6e8",
   "metadata": {},
   "source": [
    "<a id='Imports'></a>\n",
    "\n",
    "## <span style=\"font-family: Trebuchet MS; font-weight:bold;font-size:1.2em;color:#00b3e5;\">  Import all required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "636cbf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520bdcce",
   "metadata": {},
   "source": [
    "## <span style=\"font-family: Trebuchet MS; font-weight:bold;font-size:1.2em;color:#00b3e5;\">  Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e2072ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_df_details(df):\n",
    "    print(\"\\n****** Start Dataframe Details ******\")\n",
    "    print(\"Dataframe Shape is \")\n",
    "    print(df.shape)\n",
    "    print(\"Dataframe Info is \")\n",
    "    print(df.info())\n",
    "    print(\"Dataframe random records \")\n",
    "    display(df.sample(5) if len(df) > 5 else df.sample(1))\n",
    "    print(\"****** End Dataframe Details ******\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dfd89dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file and convert it to Dataframe\n",
    "def file_to_df(fqfn, file_type='csv'):\n",
    "    df = 'None'\n",
    "    if (file_type == 'csv'):\n",
    "        df = pd.read_csv(fqfn, skipinitialspace = True)\n",
    "    else:\n",
    "        raise ValueError('Unsupported filetype ', file_type)\n",
    "    print_df_details(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41e04ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Start Dataframe Details ******\n",
      "Dataframe Shape is \n",
      "(842335, 2)\n",
      "Dataframe Info is \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 842335 entries, 0 to 842334\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   Content  842308 non-null  object\n",
      " 1   Label    842335 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 12.9+ MB\n",
      "None\n",
      "Dataframe random records \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>703487</th>\n",
       "      <td>no one will ever stop me saying nigger is a ho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680481</th>\n",
       "      <td>I'm out of all my fucking meds bc my family th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370485</th>\n",
       "      <td>`  :Well that is not correct. If you are unwil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584459</th>\n",
       "      <td>Alpha Phi Alpha and Kappa Alpha Psi \\n\\nI beli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122606</th>\n",
       "      <td>== NAZIS RAUS ==  SHUT THIS NAZI NIXER UP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Content  Label\n",
       "703487  no one will ever stop me saying nigger is a ho...      0\n",
       "680481  I'm out of all my fucking meds bc my family th...      0\n",
       "370485  `  :Well that is not correct. If you are unwil...      0\n",
       "584459  Alpha Phi Alpha and Kappa Alpha Psi \\n\\nI beli...      0\n",
       "122606          == NAZIS RAUS ==  SHUT THIS NAZI NIXER UP      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** End Dataframe Details ******\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = file_to_df('E:\\Temp\\data_huang_devansh.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a02ac90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in e:\\software\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\software\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in e:\\software\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in e:\\software\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in e:\\software\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: colorama in e:\\software\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d73fecea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MSS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MSS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\MSS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2e4c5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk_stop_words = stopwords.words('english')\n",
    "print(nltk_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ead3cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234927 sha256=1beb1b4dd4bde8381249ac904ba82b4992c7db14f7f2c4f84edc96ac246694ab\n",
      "  Stored in directory: c:\\users\\mss\\appdata\\local\\pip\\cache\\wheels\\9a\\b8\\0f\\f580817231cbf59f6ade9fd132ff60ada1de9f7dc85521f857\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b354a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83e58736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Start Dataframe Details ******\n",
      "Dataframe Shape is \n",
      "(560879, 2)\n",
      "Dataframe Info is \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 560879 entries, 0 to 839518\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   Content  560879 non-null  object\n",
      " 1   Label    560879 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 12.8+ MB\n",
      "None\n",
      "Dataframe random records \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>501577</th>\n",
       "      <td>put on the lord jesus christ, and make no prov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37393</th>\n",
       "      <td>== red flag baloney ==  baloney is what your m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406013</th>\n",
       "      <td>They barely cooked anything on that entrée #MKR</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88381</th>\n",
       "      <td>Uh can you please change the name? this is not...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54023</th>\n",
       "      <td>== The lunchlady drowned last February. ==  Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Content  Label\n",
       "501577  put on the lord jesus christ, and make no prov...      0\n",
       "37393   == red flag baloney ==  baloney is what your m...      1\n",
       "406013    They barely cooked anything on that entrée #MKR      0\n",
       "88381   Uh can you please change the name? this is not...      0\n",
       "54023   == The lunchlady drowned last February. ==  Th...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** End Dataframe Details ******\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.drop_duplicates(inplace=True)\n",
    "print_df_details(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dccd6ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560879, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eadb3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d9507e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_NAME='mendeley_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7152f595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus_name</th>\n",
       "      <th>raw_sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_sentence_training</th>\n",
       "      <th>clean_sentence_EDA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>503101</th>\n",
       "      <td>mendeley_dataset</td>\n",
       "      <td>U Lil stupid ass bitch I aint fucking wit youu...</td>\n",
       "      <td>0</td>\n",
       "      <td>U Lil stupid as bitch I aint fucking wit youuuu</td>\n",
       "      <td>U Lil stupid as bitch I aint fucking wit youuuu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348833</th>\n",
       "      <td>mendeley_dataset</td>\n",
       "      <td>Thanks for advise \\n\\nThanks you for your advi...</td>\n",
       "      <td>0</td>\n",
       "      <td>Thanks advise Thanks advice use picture</td>\n",
       "      <td>Thanks advise Thanks advice use picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145444</th>\n",
       "      <td>mendeley_dataset</td>\n",
       "      <td>==Will she be the official Queen?== I cannot ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Will official Queen I cannot see Henry VIII ma...</td>\n",
       "      <td>Will official Queen I cannot see Henry VIII ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312446</th>\n",
       "      <td>mendeley_dataset</td>\n",
       "      <td>\"\\nRe the \"\"Hubbard Day\"\", I thought you might...</td>\n",
       "      <td>0</td>\n",
       "      <td>Re Hubbard Day I thought might like that I see...</td>\n",
       "      <td>Re Hubbard Day I thought might like that I see...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             corpus_name                                       raw_sentence  \\\n",
       "503101  mendeley_dataset  U Lil stupid ass bitch I aint fucking wit youu...   \n",
       "348833  mendeley_dataset  Thanks for advise \\n\\nThanks you for your advi...   \n",
       "145444  mendeley_dataset   ==Will she be the official Queen?== I cannot ...   \n",
       "312446  mendeley_dataset  \"\\nRe the \"\"Hubbard Day\"\", I thought you might...   \n",
       "\n",
       "        label                            clean_sentence_training  \\\n",
       "503101      0    U Lil stupid as bitch I aint fucking wit youuuu   \n",
       "348833      0            Thanks advise Thanks advice use picture   \n",
       "145444      0  Will official Queen I cannot see Henry VIII ma...   \n",
       "312446      0  Re Hubbard Day I thought might like that I see...   \n",
       "\n",
       "                                       clean_sentence_EDA  \n",
       "503101    U Lil stupid as bitch I aint fucking wit youuuu  \n",
       "348833            Thanks advise Thanks advice use picture  \n",
       "145444  Will official Queen I cannot see Henry VIII ma...  \n",
       "312446  Re Hubbard Day I thought might like that I see...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_name=[]\n",
    "raw_sentence=[]\n",
    "label=[]\n",
    "clean_sentence_training=[]\n",
    "clean_sentence_EDA=[]\n",
    "for row in data.values:\n",
    "    new_sentence = \"\"\n",
    "    old_sentence = row[0].split()\n",
    "    for word in old_sentence:\n",
    "        if word.find(\"https:\")==0 and word.find(\"http:\")==0 and word.find(\"www:\")==0:\n",
    "                continue\n",
    "        elif len(word) > 1 and (word.startswith('#') or word.startswith('@')):\n",
    "            new_sentence = new_sentence + \" \" + word\n",
    "        elif word not in nltk_stop_words:\n",
    "            word = re.sub('[()!?]', '', word)\n",
    "            word = re.sub('\\[.*?\\]','', word)\n",
    "            word = re.sub(\"[^A-Za-z]\",'', word)\n",
    "            base_word = wordnet_lemmatizer.lemmatize(word)\n",
    "            new_sentence = new_sentence + \" \" + base_word\n",
    "    corpus_name.append(FOLDER_NA)\n",
    "    raw_sentence.append(row[0])\n",
    "    label.append(row[1])\n",
    "    clean_sentence_training.append(new_sentence.strip())\n",
    "    if (emoji.emoji_count(new_sentence) > 0):\n",
    "        #print(emoji.emoji_list(new_sentence))\n",
    "        clean_sentence_EDA.append(emoji.replace_emoji(new_sentence.strip(), ''))\n",
    "    else:\n",
    "        clean_sentence_EDA.append(new_sentence.strip())\n",
    "    #break\n",
    "new_df = pd.DataFrame(\n",
    "    {'corpus_name': corpus_name,\n",
    "     'raw_sentence': raw_sentence,\n",
    "     'label': label,\n",
    "     'clean_sentence_training': clean_sentence_training,\n",
    "     'clean_sentence_EDA': clean_sentence_EDA\n",
    "    })\n",
    "new_df.sample(4)\n",
    "#print(f\"{content} and {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc20b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df.to_csv('clean_mendeley_hatespeech_dataset.csv')\n",
    "\n",
    "#new_df[new_df['label'] == 0].to_csv('clean_mendeley_hatespeech_dataset_0.csv')\n",
    "#new_df[new_df['label'] == 1].to_csv('clean_mendeley_hatespeech_dataset_1.csv')\n",
    "for no, chunk in enumerate(np.array_split(new_df, 6)):\n",
    "    suffix = chr(no+65)\n",
    "    chunk.to_csv(f'../../../data/english/clean-datasets/{FOLDER_NAME}/clean_mendeley_hatespeech_dataset_part{suffix}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ffe034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
