{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-23T12:16:23.294366Z",
     "iopub.status.busy": "2023-02-23T12:16:23.293713Z",
     "iopub.status.idle": "2023-02-23T12:16:30.866142Z",
     "shell.execute_reply": "2023-02-23T12:16:30.865078Z",
     "shell.execute_reply.started": "2023-02-23T12:16:23.294232Z"
    },
    "id": "nlyBcBLXvpZK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#train split and fit models\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-23T12:16:30.868847Z",
     "iopub.status.busy": "2023-02-23T12:16:30.868115Z",
     "iopub.status.idle": "2023-02-23T12:16:31.526100Z",
     "shell.execute_reply": "2023-02-23T12:16:31.525170Z",
     "shell.execute_reply.started": "2023-02-23T12:16:30.868806Z"
    },
    "id": "21Hk8O5CvpZN"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./model_ready_combined.csv')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-23T12:16:31.529620Z",
     "iopub.status.busy": "2023-02-23T12:16:31.529284Z",
     "iopub.status.idle": "2023-02-23T12:16:31.654245Z",
     "shell.execute_reply": "2023-02-23T12:16:31.652980Z",
     "shell.execute_reply.started": "2023-02-23T12:16:31.529593Z"
    },
    "id": "jGT2u_VPvpZN"
   },
   "outputs": [],
   "source": [
    "dataset.dropna(inplace=True)\n",
    "dataset.drop_duplicates(inplace=True)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-23T12:16:31.657573Z",
     "iopub.status.busy": "2023-02-23T12:16:31.657156Z",
     "iopub.status.idle": "2023-02-23T12:16:31.671829Z",
     "shell.execute_reply": "2023-02-23T12:16:31.670610Z",
     "shell.execute_reply.started": "2023-02-23T12:16:31.657534Z"
    },
    "id": "X-3Fz7eBvpZO"
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAYrKy4MvpZO"
   },
   "source": [
    "### class values - \n",
    "+ label == 0 - Not Hateful/ Non-Risky\n",
    "+ label == 1 - Potentially Risky \n",
    "+ label == 2 - hateful / Risky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-23T12:16:31.674243Z",
     "iopub.status.busy": "2023-02-23T12:16:31.673883Z",
     "iopub.status.idle": "2023-02-23T12:16:31.678757Z",
     "shell.execute_reply": "2023-02-23T12:16:31.677772Z",
     "shell.execute_reply.started": "2023-02-23T12:16:31.674207Z"
    },
    "id": "nYo-fZ6vvpZP"
   },
   "outputs": [],
   "source": [
    "label_lst = [\"Non-Risky\", \"Potentially Risky\", \"Risky\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3ZQ1mUpvpZP"
   },
   "source": [
    "# Hybrid Model (BERT+LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "334970dc3b2944e7be5731864b220b22",
      "26b8e92955c048218f9f2eceadc80bd0",
      "d0191474da624349817b8b85fb7052f0",
      "985107105935404b9043407d29f36181"
     ]
    },
    "execution": {
     "iopub.execute_input": "2023-03-03T05:44:49.185167Z",
     "iopub.status.busy": "2023-03-03T05:44:49.184815Z",
     "iopub.status.idle": "2023-03-03T08:06:30.131748Z",
     "shell.execute_reply": "2023-03-03T08:06:30.130424Z",
     "shell.execute_reply.started": "2023-03-03T05:44:49.185090Z"
    },
    "id": "Na4pgwufvpZQ",
    "outputId": "001f267d-0001-46e1-a68b-a7cded5164b0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout, GlobalMaxPooling1D, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('./train-data.csv')\n",
    "test = pd.read_csv('./test-data.csv')\n",
    "train.drop_duplicates(inplace=True)\n",
    "test.dropna(inplace=True)\n",
    "train.drop_duplicates(inplace=True)\n",
    "test.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Define BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# Tokenize text data\n",
    "MAX_LEN = 128\n",
    "X_train_tokenized = tokenizer.batch_encode_plus(\n",
    "    train['clean training'].tolist(),\n",
    "    max_length=MAX_LEN,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "X_test_tokenized = tokenizer.batch_encode_plus(\n",
    "    test['clean training'].tolist(),\n",
    "    max_length=MAX_LEN,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# Create input tensors\n",
    "attention_masks = np.array(X_train_tokenized['attention_mask'])\n",
    "X_train = np.array(X_train_tokenized['input_ids'])\n",
    "X_test = np.array(X_test_tokenized['input_ids'])\n",
    "\n",
    "y_train = np.array(train['label'])\n",
    "y_test = np.array(test['label'])\n",
    "\n",
    "# Split combined validation-test set into separate validation and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train)\n",
    "\n",
    "# Define input layer for BERT model\n",
    "input_layer = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\n",
    "\n",
    "# Connect tokenizer output to BERT model\n",
    "bert_output = bert_model(input_layer)[0]\n",
    "\n",
    "# Define LSTM layer\n",
    "reshape_layer = tf.keras.layers.Reshape((1, -1))(bert_output)\n",
    "lstm_layer = Bidirectional(LSTM(128, return_sequences=True))(reshape_layer)\n",
    "\n",
    "# Define pooling layer\n",
    "pooling_layer = GlobalMaxPooling1D()(lstm_layer)\n",
    "\n",
    "# Define output layer\n",
    "output_layer = Dense(3, activation='softmax')(pooling_layer)\n",
    "\n",
    "# Define the model\n",
    "model_1 = Model(inputs=[input_layer], outputs=[output_layer])\n",
    "\n",
    "# Compile the model\n",
    "model_1.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=1e-5), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train model\n",
    "earlyStop = EarlyStopping(monitor='val_accuracy', patience=2, \n",
    "                        verbose=1, mode='max',restore_best_weights=True)\n",
    "batch_size = 1600  # Set batch size to the same value as used in training\n",
    "\n",
    "history = model_1.fit(\n",
    "    x=X_train,\n",
    "    y=tf.keras.utils.to_categorical(y_train),\n",
    "    validation_data=(X_val, tf.keras.utils.to_categorical(y_val)),\n",
    "    epochs=5,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[earlyStop]\n",
    ")\n",
    "\n",
    "# Evaluate model on test set\n",
    "\n",
    "loss, accuracy = model_1.evaluate([X_test], tf.keras.utils.to_categorical(y_test, num_classes=3), batch_size=batch_size)\n",
    "print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T08:15:31.234695Z",
     "iopub.status.busy": "2023-03-03T08:15:31.234258Z",
     "iopub.status.idle": "2023-03-03T08:22:18.730816Z",
     "shell.execute_reply": "2023-03-03T08:22:18.729732Z",
     "shell.execute_reply.started": "2023-03-03T08:15:31.234659Z"
    },
    "id": "GIoN5chRvpZR",
    "outputId": "68bebd10-24ba-41dd-b032-cc21bd1931cd"
   },
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "batch_size = 16  # Set batch size to the same value as used in training\n",
    "loss, accuracy = model_1.evaluate([X_test], tf.keras.utils.to_categorical(y_test, num_classes=3), batch_size=batch_size)\n",
    "print('Test accuracy:', accuracy)\n",
    "\n",
    "y_pred = model_1.predict(X_test)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1) \n",
    "\n",
    "# Print Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(f\"\\nClassification Report :\\n{classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAhMr9FRvpZS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T08:22:18.733634Z",
     "iopub.status.busy": "2023-03-03T08:22:18.732724Z",
     "iopub.status.idle": "2023-03-03T08:22:22.071235Z",
     "shell.execute_reply": "2023-03-03T08:22:22.070177Z",
     "shell.execute_reply.started": "2023-03-03T08:22:18.733596Z"
    },
    "id": "TTEurBpJvpZS",
    "outputId": "e99291f4-1533-4b33-fb37-ca45cda0afc2"
   },
   "outputs": [],
   "source": [
    "# Make prediction on user input\n",
    "text = 'This is a hateful and offensive message.'\n",
    "text_tokenized = tokenizer.encode_plus(\n",
    "    text,\n",
    "    max_length=MAX_LEN,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "prediction = model_1.predict([text_tokenized['input_ids'].numpy() ])\n",
    "\n",
    "print('Prediction:', np.argmax(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T08:22:22.073431Z",
     "iopub.status.busy": "2023-03-03T08:22:22.072787Z",
     "iopub.status.idle": "2023-03-03T08:22:32.374043Z",
     "shell.execute_reply": "2023-03-03T08:22:32.373059Z",
     "shell.execute_reply.started": "2023-03-03T08:22:22.073388Z"
    },
    "id": "Rah8xFt8vpZS"
   },
   "outputs": [],
   "source": [
    "model_1.save(\"DL_model_Bart_Lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T08:22:32.377220Z",
     "iopub.status.busy": "2023-03-03T08:22:32.376752Z",
     "iopub.status.idle": "2023-03-03T08:22:36.157871Z",
     "shell.execute_reply": "2023-03-03T08:22:36.156871Z",
     "shell.execute_reply.started": "2023-03-03T08:22:32.377182Z"
    },
    "id": "LxzXP1mXvpZS"
   },
   "outputs": [],
   "source": [
    "model_1.save(\"DL_model_Bart_Lstm_2.hdf5\", include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwuZu0_0vpZT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhSNIi0dvpZT"
   },
   "source": [
    "## How to load the model : TFBertModel and include optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T08:37:46.796423Z",
     "iopub.status.busy": "2023-03-03T08:37:46.795982Z",
     "iopub.status.idle": "2023-03-03T08:41:20.507889Z",
     "shell.execute_reply": "2023-03-03T08:41:20.506945Z",
     "shell.execute_reply.started": "2023-03-03T08:37:46.796385Z"
    },
    "id": "2dbjiilHvpZT",
    "outputId": "a11933a4-ad34-4826-810b-de5e309050ad"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "loaded_model = load_model(\"/kaggle/working/DL_model_Bart_Lstm_2.hdf5\", custom_objects = {'TFBertModel': TFBertModel})\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(lr=1e-5)\n",
    "loss = 'categorical_crossentropy'\n",
    "loaded_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4k0ZNLxovpZT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# Load data\n",
    "test = pd.read_csv('/kaggle/input/hatefull-and-offensive-language/test-data.csv')\n",
    "test.dropna(inplace=True)\n",
    "test.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Define BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# Tokenize text data\n",
    "MAX_LEN = 128\n",
    "X_test_tokenized = tokenizer.batch_encode_plus(\n",
    "    test['clean training'].tolist(),\n",
    "    max_length=MAX_LEN,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# Create input tensors\n",
    "attention_masks = np.array(X_test_tokenized['attention_mask'])\n",
    "X_test = np.array(X_test_tokenized['input_ids'])\n",
    "y_test = np.array(test['label'])\n",
    "\n",
    "batch_size = 16  # Set batch size to the same value as used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyCOhsszvpZT"
   },
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "batch_size = 16  # Set batch size to the same value as used in training\n",
    "loss, accuracy = model_1.evaluate([X_test], tf.keras.utils.to_categorical(y_test, num_classes=3), batch_size=batch_size)\n",
    "print('Test accuracy:', accuracy)\n",
    "\n",
    "y_pred = model_1.predict(X_test)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1) \n",
    "\n",
    "# Print Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(f\"\\nClassification Report :\\n{classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XmtQjf39vpZT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T08:41:20.510374Z",
     "iopub.status.busy": "2023-03-03T08:41:20.509920Z",
     "iopub.status.idle": "2023-03-03T08:41:23.928262Z",
     "shell.execute_reply": "2023-03-03T08:41:23.927249Z",
     "shell.execute_reply.started": "2023-03-03T08:41:20.510340Z"
    },
    "id": "hS5pNfktvpZU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# Define BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "# Make prediction on user input\n",
    "text = 'I do not like differently abled'\n",
    "text_tokenized = tokenizer.encode_plus(\n",
    "    text,\n",
    "    max_length=MAX_LEN,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "prediction = loaded_model.predict([text_tokenized['input_ids'].numpy() ])\n",
    "print(prediction)\n",
    "print('Prediction:', np.argmax(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T08:41:23.930691Z",
     "iopub.status.busy": "2023-03-03T08:41:23.929980Z",
     "iopub.status.idle": "2023-03-03T08:41:24.211015Z",
     "shell.execute_reply": "2023-03-03T08:41:24.209966Z",
     "shell.execute_reply.started": "2023-03-03T08:41:23.930653Z"
    },
    "id": "kxqAtzNLvpZU",
    "outputId": "9279376f-7fe3-413a-85ed-7b0f5249314f"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1yj2QdqvpZU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRZpUbqBvpZU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKs-24CsvpZU"
   },
   "source": [
    "# DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-02T13:49:04.508922Z",
     "iopub.status.busy": "2023-03-02T13:49:04.508576Z",
     "iopub.status.idle": "2023-03-02T14:59:09.970882Z",
     "shell.execute_reply": "2023-03-02T14:59:09.969945Z",
     "shell.execute_reply.started": "2023-03-02T13:49:04.508892Z"
    },
    "id": "uPZ2Lr8nvpZU",
    "outputId": "d21840f6-919c-45b1-aa4b-87eec89cb91b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout, GlobalMaxPooling1D, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/kaggle/input/hatefull-and-offensive-language/train-data.csv')\n",
    "test = pd.read_csv('/kaggle/input/hatefull-and-offensive-language/test-data.csv')\n",
    "train.drop_duplicates(inplace=True)\n",
    "test.dropna(inplace=True)\n",
    "train.drop_duplicates(inplace=True)\n",
    "test.dropna(inplace=True)\n",
    "\n",
    "# Define BERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize text data\n",
    "MAX_LEN = 128\n",
    "X_train_tokenized = tokenizer.batch_encode_plus(\n",
    "    train['clean training'].tolist(),\n",
    "    max_length=MAX_LEN,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "X_test_tokenized = tokenizer.batch_encode_plus(\n",
    "    test['clean training'].tolist(),\n",
    "    max_length=MAX_LEN,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# Create input tensors\n",
    "attention_masks = np.array(X_train_tokenized['attention_mask'])\n",
    "X_train = np.array(X_train_tokenized['input_ids'])\n",
    "X_test = np.array(X_test_tokenized['input_ids'])\n",
    "\n",
    "y_train = np.array(train['label'])\n",
    "y_test = np.array(test['label'])\n",
    "\n",
    "# Define input layer for BERT model\n",
    "input_layer = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\n",
    "\n",
    "# Connect tokenizer output to BERT model\n",
    "bert_output = bert_model(input_layer)[0]\n",
    "\n",
    "# Define LSTM layer\n",
    "lstm_layer = Bidirectional(LSTM(128, return_sequences=True))(bert_output)\n",
    "\n",
    "# Define pooling layer\n",
    "pooling_layer = GlobalMaxPooling1D()(lstm_layer)\n",
    "\n",
    "# Define output layer\n",
    "output_layer = Dense(3, activation='softmax')(pooling_layer)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input_layer], outputs=[output_layer])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=1e-5), metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Evaluate model on test set\n",
    "batch_size = 16  # Set batch size to the same value as used in training\n",
    "loss, accuracy = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print('Test accuracy:', accuracy)\n",
    "\n",
    "# Use model to make predictions\n",
    "text = 'This is a hateful tweet'\n",
    "text_tokenized = tokenizer.encode_plus(\n",
    "    text,\n",
    "    max_length=MAX_LEN,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "text_input = np.array(text_tokenized['input_ids']).reshape(1, -1)\n",
    "prediction = model.predict(text_input)\n",
    "predicted_label = np.argmax(prediction)\n",
    "print('Predicted label:', predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-02T15:28:13.061405Z",
     "iopub.status.busy": "2023-03-02T15:28:13.060575Z",
     "iopub.status.idle": "2023-03-02T15:29:37.495996Z",
     "shell.execute_reply": "2023-03-02T15:29:37.494898Z",
     "shell.execute_reply.started": "2023-03-02T15:28:13.061370Z"
    },
    "id": "_IopHpadvpZV",
    "outputId": "07c55993-7811-4c60-da2c-b7a682accf58"
   },
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1) \n",
    "\n",
    "# Print Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(f\"\\nClassification Report :\\n{classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-02T15:29:37.498491Z",
     "iopub.status.busy": "2023-03-02T15:29:37.498120Z",
     "iopub.status.idle": "2023-03-02T15:29:37.764976Z",
     "shell.execute_reply": "2023-03-02T15:29:37.764310Z",
     "shell.execute_reply.started": "2023-03-02T15:29:37.498454Z"
    },
    "id": "tWLOFdkPvpZV",
    "outputId": "91931a24-b92c-431a-a709-e7fd9e0901b7"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-02T15:29:37.767277Z",
     "iopub.status.busy": "2023-03-02T15:29:37.766279Z",
     "iopub.status.idle": "2023-03-02T15:29:38.992974Z",
     "shell.execute_reply": "2023-03-02T15:29:38.992007Z",
     "shell.execute_reply.started": "2023-03-02T15:29:37.767234Z"
    },
    "id": "1W5WzcmvvpZV"
   },
   "outputs": [],
   "source": [
    "model.save(\"DL_model_DistilBert_Lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-26T19:13:13.948858Z",
     "iopub.status.busy": "2023-02-26T19:13:13.948061Z",
     "iopub.status.idle": "2023-02-26T19:14:05.331087Z",
     "shell.execute_reply": "2023-02-26T19:14:05.328917Z",
     "shell.execute_reply.started": "2023-02-26T19:13:13.948796Z"
    },
    "id": "F26RGgoqvpZV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3svDABhjvpZV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7qr-hDMvpZV"
   },
   "source": [
    "## How to load the model : TFDistilBertModel and include optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqkAbD8IvpZW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "ee041a41409f49e9aa1772526026b08d",
      "25f5314cd40a4bd4aaced7a129bcc4bd",
      "4a97498fcd1c4afd8952573e8eabde75",
      "bcb01a7d55e04fefadee33c96433053f"
     ]
    },
    "execution": {
     "iopub.execute_input": "2023-03-02T13:31:58.434753Z",
     "iopub.status.busy": "2023-03-02T13:31:58.433912Z",
     "iopub.status.idle": "2023-03-02T13:32:56.553142Z",
     "shell.execute_reply": "2023-03-02T13:32:56.552115Z",
     "shell.execute_reply.started": "2023-03-02T13:31:58.434718Z"
    },
    "id": "psDZkGIivpZW",
    "outputId": "b456e361-ed0b-428b-dee3-fd3ba2d431a0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout, GlobalMaxPooling1D, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/hatefull-and-offensive-language/test-data.csv')\n",
    "test.drop_duplicates(inplace=True)\n",
    "test.dropna(inplace=True)\n",
    "X_test = test['clean training']\n",
    "y_test = test['label']\n",
    "\n",
    "# Define BERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize text data\n",
    "MAX_LEN = 128\n",
    "X_test_tokenized = tokenizer.batch_encode_plus(\n",
    "    X_test.tolist(),\n",
    "    max_length=MAX_LEN,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# Create input tensors\n",
    "X_test = np.array(X_test_tokenized['input_ids'])\n",
    "attention_masks = np.array(X_test_tokenized['attention_mask'])\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-02T13:32:56.555663Z",
     "iopub.status.busy": "2023-03-02T13:32:56.555273Z",
     "iopub.status.idle": "2023-03-02T13:35:34.765310Z",
     "shell.execute_reply": "2023-03-02T13:35:34.764240Z",
     "shell.execute_reply.started": "2023-03-02T13:32:56.555628Z"
    },
    "id": "lfflbtU8vpZW"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "\n",
    "# Evaluate model on test set\n",
    "batch_size = 16  # Set batch size to the same value as used in training\n",
    "\n",
    "from keras.models import load_model\n",
    "loaded_model = load_model(\"/kaggle/working/DL_model_DistilBert_Lstm.h5\", custom_objects = {'TFDistilBertModel': TFDistilBertModel})\n",
    "\n",
    "# Evaluate model on test set\n",
    "loss, accuracy = loaded_model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "# Get predicted labels\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Print Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(f\"\\nClassification Report :\\n{classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-Qe1LFtvpZW"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# Define BERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "# Make prediction on user input\n",
    "text = 'I do not like differently abled'\n",
    "text_tokenized = tokenizer.encode_plus(\n",
    "    text,\n",
    "    max_length=MAX_LEN,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "prediction = loaded_model.predict([text_tokenized['input_ids'].numpy() ])\n",
    "print(prediction)\n",
    "print('Prediction:', np.argmax(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwLL0g01vpZW"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
